# -*- coding: utf-8 -*-
"""Employee salary prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11sDnrB2qRxt5HTTg0yHO1hr3sDbmtqLl
"""

#employee salary prediction

import pandas as pd

data = pd.read_csv("/adult.csv")

data

data.shape

data.head()

data.head(7)

data.tail(4)

#null value checking .if there is how true else false
data.isna()

data.isna().sum()

data.head(3)

#to understand the categories under each column for eg: in gender: male and female
print(data.occupation.value_counts())

print(data.gender.value_counts())

print(data.education.value_counts())

print(data['marital-status'].value_counts())

print(data['education'].value_counts())

print(data['workclass'].value_counts())

print(data['gender'].value_counts())

#replace ? with some title other

data.occupation.replace({'?':'Others'},inplace=True)

print(data.occupation.value_counts())

data.workclass.replace({'?':'NotListed'},inplace=True)

print(data['workclass'].value_counts())

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']

print(data['workclass'].value_counts())

data.shape

data=data[data['education']!='5th-6th']
data=data[data['education']!='Preschool']
data=data[data['education']!='1st-4th']

print(data['education'].value_counts())

data.shape

#redundancy
data.drop(columns=['education'],inplace=True)

data

#outlier
import matplotlib.pyplot as plt
plt.boxplot(data['age'])
plt.show()

#outlier
import matplotlib.pyplot as plt
plt.boxplot(data['hours-per-week'])
plt.show()

data=data[(data['age']<=75) &(data['age']>=17)]

plt.boxplot(data['age'])
plt.show()

plt.boxplot(data['hours-per-week'])
plt.show()

# done with preprocessing..did for some row or col for example
#next step is encoding , covert the categorical value to numerical value
print(data.workclass)

print(data['workclass'].value_counts())

#label Encoding
from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
data['workclass']=encoder.fit_transform(data['workclass'])
data['marital-status']=encoder.fit_transform(data['marital-status'])
data['occupation']=encoder.fit_transform(data['occupation'])
data['relationship']=encoder.fit_transform(data['relationship'])
data['race']=encoder.fit_transform(data['race'])
data['gender']=encoder.fit_transform(data['gender'])
data['native-country']=encoder.fit_transform(data['native-country'])
data

#split the data for train/test
#take 2 variable x and y , remove the last colum.because it is the output

x=data.drop(columns=['income'])
y=data['income']
x

y

data

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
x=scaler.fit_transform(x)
x

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y, test_size=0.2,random_state=23,stratify=y)

xtrain

#machine learning algorithm, here supervised algo we use
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(xtrain,ytrain)#input and output training data
predict=knn.predict(xtest)
predict

#we need to evaluate the model's prediction, we calculate accuracy
from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict)

#machine learning algorithm, here supervised algo we use
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(xtrain,ytrain)#input and output training data
predict1=lr.predict(xtest)
predict1

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict1)

from sklearn.neural_network import MLPClassifier
clf=MLPClassifier(solver='adam',hidden_layer_sizes=(5,2),random_state=2,max_iter=2000)
clf.fit(xtrain,ytrain)
predict2=clf.predict(xtest)
predict2

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict2)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

from sklearn.preprocessing import StandardScaler

# Define models
models = {
    "LogisticRegression": LogisticRegression(),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

# Dictionary to hold accuracy results
results = {}

# Train and evaluate each model
for name, model in models.items():
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    pipe.fit(xtrain, ytrain)
    ypred = pipe.predict(xtest)
    acc = accuracy_score(ytest, ypred)
    results[name] = acc

    print(f"{name} Accuracy: {acc:.4f}")
    print(classification_report(ytest, ypred))

import matplotlib.pyplot as plt
plt.bar(results.keys(), results.values(), color='skyblue')
plt.ylabel('Accuracy Score')
plt.title('Model Comparison')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()